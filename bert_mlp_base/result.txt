sentence_len=200
epoch: 105  loss= 0.0036691917785660576  time= 134.90190148353577  s   lamda= tensor([0.1719], device='cuda:0')
train: accuracy=0.78897  micro_f1=0.95095  macro_f1=0.94202
 val: accuracy=0.77540  micro_f1=0.94161  macro_f1=0.93050
 test: accuracy=0.76800  micro_f1=0.94005  macro_f1=0.93002


********************************************************
* 时间：2022-03-31 04:40:31
* sentence:300
* batch_size:128
* best_acc_resultepoch:105
train:0.65082		0.91130	0.90247
 val:0.74720		0.93488	0.92618
 test:0.73840		0.93252	0.92323
********************************************************
********************************************************
* 时间：2022-03-31 16:39:47
* sentence:300
* batch_size:128
* best_acc_resultepoch:100
train:0.88905		0.97673	0.96722
 val:0.79600		0.94174	0.93051
 test:0.78400		0.94023	0.92948
********************************************************

********************************************************
* 时间：2022-04-01 10:07:35
* sentence:300
* batch_size:256
* best_acc_result
epoch:208
train:0.89643		0.97797	0.96841
 val:0.81480		0.94683	0.93725
 test:0.80480		0.94450	0.93610
********************************************************


heirarchy_label_graph
********************************************************
epoch: 101  loss= 0.0021579199392478205  time= 166.13915300369263  s   lamda= tensor([0.], device='cuda:0')
train: accuracy=0.86798  micro_f1=0.97010  macro_f1=0.96016
 val: accuracy=0.80960  micro_f1=0.94679  macro_f1=0.93611
 test: accuracy=0.80580  micro_f1=0.94519  macro_f1=0.93579
 ********************************************************

cooccurence_cosin_label_graph
********************************************************
* 时间：2022-04-02 00:46:58
* sentence:300
* batch_size:256
* best_acc_result
epoch:55
train:0.87677		0.97232	0.96260
 val:0.80980		0.94807	0.93916
 test:0.80800		0.94547	0.93502
********************************************************


null
********************************************************
* 时间：2022-04-02 08:06:14
* sentence:300
* batch_size:256
* best_acc_result
epoch:55
train:0.85935		0.96668	0.95665
 val:0.80780		0.94643	0.93538
 test:0.80180		0.94356	0.93290
********************************************************


***********************use_label==*********************************
* 时间：2022-04-02 14:30:02
* sentence:300
* batch_size:512
* best_acc_result
epoch:59
train:0.81632		0.95383	0.94267
 val:0.78460		0.94201	0.92993
 test:0.78600		0.94028	0.92949
********************************************************
***********************use_label==*********************************
* 时间：2022-04-03 00:38:30
* sentence:300
* batch_size:512
* best_acc_result
epoch:59
train:0.81632		0.95383	0.94267
 val:0.78460		0.94201	0.92993
 test:0.78600		0.94028	0.92949
********************************************************
***********************use_label==attention_cooccurence_cosin_label_graph.pth*********************************
* 时间：2022-04-03 21:39:43
* sentence:300
* batch_size:256
* best_acc_result
epoch:54
train:0.87380		0.97155	0.96203
 val:0.81280		0.94667	0.93597
 test:0.80560		0.94418	0.93459
********************************************************
***********************use_label==cooccurence_cosin_label_graph.pth***filter-->log-->/max_log******************************
up_boundry=1024
down_boundry=16
* 时间：2022-04-08 00:15:34
* sentence:300
* batch_size:256
* best_acc_result
epoch:53
train:0.84140		0.96570	0.95723
 val:0.78640		0.94259	0.93228
 test:0.78280		0.94159	0.93216
********************************************************
只有lamda
***********************use_label==cooccurence_cosin_label_graph.pth***filter-->log-->/max_log******************************
* 时间：2022-04-08 15:39:46
* sentence:300
* batch_size:256
* best_acc_result
epoch:34
train:0.83478		0.96344	0.95466
 val:0.78600		0.94223	0.93175
 test:0.79160		0.94318	0.93332
********************************************************
加了mu
***********************use_label==cooccurence_cosin_label_graph.pth***filter-->log-->/max_log******************************
* 时间：2022-04-09 00:18:07
* sentence:300
* batch_size:256
* best_acc_result
epoch:63
train:0.84655		0.96378	0.95392
 val:0.79980		0.94436	0.93395
 test:0.79340		0.94147	0.93059
********************************************************
加了mu  加了epoch  初始化0.5
***********************use_label==cooccurence_cosin_label_graph.pth***filter-->log-->/max_log******************************
* 时间：2022-04-10 05:49:24
* sentence:300
* batch_size:256
* best_acc_result
epoch:71
train:0.87275		0.97142	0.96159
 val:0.80820		0.94642	0.93596
 test:0.80160		0.94514	0.93525
********************************************************


mu=1-lamda
***********************use_label==cooccurence_cosin_label_graph.pth***filter-->log-->/max_log******************************
* 时间：2022-04-10 19:35:25
* sentence:300
* batch_size:256
* best_acc_result
epoch:40
train:0.85608		0.96709	0.95752
 val:0.80540		0.94551	0.93440
 test:0.80300		0.94494	0.93510
********************************************************






coocurrence gcn
*************************************************************************************
过滤高低频，取对数，只用共现关系作为图结构，共现频率作为权重
epoch: 51  loss= 0.0021503482408418207  time= 164.69211268424988
train: accuracy=0.86998  micro_f1=0.97073  macro_f1=0.96109
 val: accuracy=0.81080  micro_f1=0.94708  macro_f1=0.93694
 test: accuracy=0.80560  micro_f1=0.94565  macro_f1=0.93514



coocurrence gat
*************************************************************************************
过滤高低频，取对数，只用共现关系作为图结构
epoch: 43  loss= 0.0021493858457572617  time= 164.52485728263855
train: accuracy=0.87070  micro_f1=0.97041  macro_f1=0.96082
 val: accuracy=0.81180  micro_f1=0.94847  macro_f1=0.93759
 test: accuracy=0.80840  micro_f1=0.94538  macro_f1=0.93563

层级gcn
***********************************************
* 时间：2022-05-08 22:09:29
* sentence:300
* batch_size:256
* best_acc_result
epoch:59
train:0.87940		0.97287	0.96338
 val:0.81520		0.94781	0.93751
 test:0.80820		0.94521	0.93530
********************************************************


***********************************************
* 时间：2022-05-09 01:18:56
* sentence:300
* batch_size:256
* best_acc_result
epoch:54
train:0.87718		0.97230	0.96251
 val:0.81480		0.94922	0.93766
 test:0.80420		0.94602	0.93562
********************************************************


